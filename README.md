# IAS-Project-Classification-of-generalized-audio-
Divide the data into 70% for training and 30% for testing. Extract all the audio features (time and frequency domain). Visualize the feature space in 3D using PCA and report the number of coefficients which offer at least 80% of variance. Train the kNN classifier on the train set to classify the test set.
Project IAS

For my project, I have decided to use the Cricket, Pig, and Sipping classes among the given 50 classes. Following the instructions, I have divided the sets into various train and test subsets.

The program's structure consists of four main blocks:

Feature extraction of temporal and frequency features.
Representation of the above features through PCA.
Calculation of KNN on all feature groups.
Optimization.
Main:
This code performs audio signal classification using the K-Nearest Neighbors (KNN) algorithm on three different audio features. It sets the window length and step length for dividing the audio signal into temporal windows for feature extraction. The program then defines the paths to the directories containing the training and test audio files for each audio signal class. The code proceeds with extracting audio features (temporal, spectral, and combined) from the training and test audio files using the "extract_features" function. Subsequently, the extracted audio features are combined into a single array for training and testing, respectively. The program then performs a Principal Component Analysis (PCA) visualization of the extracted audio features. Next, a vector of k values is defined to be used in the KNN algorithm for classification. The KNN algorithm is then executed three times on three different combinations of audio features (temporal, spectral, and combined), using their respective class labels. Finally, an optimization of the KNN algorithm is performed on the temporal features, which have shown the best performance.

Extract Features:
The "extract_features" function takes three directories, a file extension, window length, and step length as inputs. Through three for loops, the function extracts audio features from the audio tracks in the three directories, each representing a different class. Specifically, a set of temporal domain features (Energy, EEntropy, ZCR) and a set of frequency domain features (Centroid, Spread, Entropy, Roll, Flux, mfccs) are extracted. The feature sets are stored in separate matrices for each class, both in the temporal and frequency domains. Finally, the feature sets in the temporal and frequency domains are separately normalized and then combined into a single set of normalized features, which is returned as the output of the function.

PCA:
The "pca_visualization" function performs Principal Component Analysis (PCA) on the data contained in the "AllFeats" matrix and returns the number of coefficients with at least 80% of the total variance (10 in this case). The function utilizes MATLAB's "pca()" function to calculate the principal components of the data. It then visualizes the data in the first three principal components using the variable "SCORE" returned by the function. The function calculates the variance explained by the principal components and returns the number of coefficients (num_coeffs) with at least 80% variance.

Knn:
The "kNN" function takes inputs such as k, features, labels of training and test data, and an "idx" parameter used for result visualization. The function starts by creating an empty vector "rate" to store the recognition rates for each provided k value (k=[1 5 10 15 20 50 100]). For each k value, the function creates a k-NN model using MATLAB's "fitcknn()" function and the training data (features1 and label1). Once the model is trained, it performs classification on the test data (features2) using the "predict()" function. The model's performance is measured by comparing the predicted labels with the true labels. For each correctly predicted label, a "correct" counter is incremented. Finally, the recognition rate is calculated as the ratio of the number of correct labels to the total number of labels in the test dataset, multiplied by 100 to obtain the result in percentage. The recognition rate is added to the "rate" vector. After the for loop iterates through all provided k values, the function finds the maximum value in the "rate" vector and saves the corresponding index in "b". A new k-NN model is then created using the k value corresponding to the highest recognition rate found, and classification is performed on the test data using this model. The function returns the predicted labels for the test data, along with the maximum recognition rate found. Finally, the function visualizes the results, showing the maximum recognition rate and the corresponding k value. A graph is also created to show the model's performance for each considered k value. This function implements the k-NN algorithm for classifying objects into two classes and returns the predicted labels for the test data, along with the maximum recognition rate and corresponding k value.

Optimization:
The function takes several arguments, including window sizes (window_sizes), step length (stepLength), and the value of k for KNN (k). The code starts with the training phase, where data (allFeatstrain, allFreqtrain, allTimetrain, trainlabeltime, trainlabelfrequency, trainlabelalltogether) is extracted from the specified directories (dir1, dir2, dir3) for different window sizes (window_sizes). Then, the function proceeds to the testing phase, where data (allFeatstest, allFreqtest, allTimeTest, testlabeltime, testlabelfrequency, testlabelalltogether) is extracted from the test directories (dir4, dir5, dir6) using the same window sizes used in the training phase. Finally, the function executes the KNN algorithm on the temporal features (allTimetrain and allTimeTest), as they have shown to be the best-performing feature group, using k as a parameter, and prints the results with the plot_kNN function.
